{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPuVNfX7xOEY","executionInfo":{"status":"ok","timestamp":1727663621017,"user_tz":360,"elapsed":68118,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"cc644dd1-5461-4797-ada5-5660804122f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/MS/assignment2/cs6353/datasets\n","--2024-09-30 02:33:31--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  52.5MB/s    in 3.2s    \n","\n","2024-09-30 02:33:34 (50.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n","/content/drive/MyDrive/MS/assignment2\n"]}],"source":["# # This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# # assignment folder, e.g. 'cs6353/assignments/assignment2/'\n","# FOLDERNAME = 'assignment2'\n","# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# # Now that we've mounted your Drive, this ensures that\n","# # the Python interpreter of the Colab VM can load\n","# # python files from within it.\n","# import sys\n","# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# This downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd /content/drive/MyDrive/MS/assignment2/cs6353/datasets\n","!bash get_datasets.sh\n","%cd ../..\n","\n","# Install requirements from colab_requirements.txt\n","# TODO: Please change your path below to the colab_requirements.txt file\n","# ! python -m pip install -r /content/drive/My\\ Drive/$FOLDERNAME/colab_requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"yWcYWZJjxOEZ"},"source":["# Softmax exercise\n","\n","*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://utah.instructure.com/courses/919972/assignments/12590082) on the course website.*\n","\n","This exercise is analogous to the SVM exercise. You will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAmZ9r7OxOEZ"},"outputs":[],"source":["from __future__ import print_function\n","import random\n","import numpy as np\n","from cs6353.data_utils import load_CIFAR10\n","import matplotlib.pyplot as plt\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVnYUL5IxOEZ","executionInfo":{"status":"ok","timestamp":1727663626517,"user_tz":360,"elapsed":4758,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"66c8b4ee-c472-4bcb-9501-84acb0f1372d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train data shape:  (49000, 3073)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3073)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3073)\n","Test labels shape:  (1000,)\n","dev data shape:  (500, 3073)\n","dev labels shape:  (500,)\n"]}],"source":["def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the linear classifier. These are the same steps as we used for the\n","    SVM, but condensed to a single function.\n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    cifar10_dir = 'cs6353/datasets/cifar-10-batches-py'\n","\n","    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n","\n","    # subsample the data\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    mask = np.random.choice(num_training, num_dev, replace=False)\n","    X_dev = X_train[mask]\n","    y_dev = y_train[mask]\n","\n","    # Preprocessing: reshape the image data into rows\n","    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","\n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis = 0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","    X_dev -= mean_image\n","\n","    # add bias dimension and transform into columns\n","    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n","\n","\n","# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","try:\n","   del X_train, y_train\n","   del X_test, y_test\n","   print('Clear previously loaded data.')\n","except:\n","   pass\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('dev data shape: ', X_dev.shape)\n","print('dev labels shape: ', y_dev.shape)"]},{"cell_type":"markdown","metadata":{"id":"NC0xTtjsxOEa"},"source":["## Softmax Classifier\n","\n","Your code for this section will all be written inside **cs6353/classifiers/softmax.py**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AG38ITLIxOEa","executionInfo":{"status":"ok","timestamp":1727663627761,"user_tz":360,"elapsed":1250,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"c5872acc-6a5d-4447-fed1-35bdf583573a"},"outputs":[{"output_type":"stream","name":"stdout","text":["loss: 2.342099\n","sanity check: 2.302585\n"]}],"source":["# First implement the naive softmax loss function with nested loops.\n","# Open the file cs6353/classifiers/softmax.py and implement the\n","# softmax_loss_naive function.\n","\n","from cs6353.classifiers.softmax import softmax_loss_naive\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(3073, 10) * 0.0001\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.1).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.1)))"]},{"cell_type":"markdown","metadata":{"id":"_Sv6RLJrxOEa"},"source":["## Inline Question 1:\n","Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n","\n","**Your answer:**\n","\n","Because the model has not been trained and learned anything. In other words, the model's predictions are random and hence making the score for each class having roughly the same probability. Since there are 10 classes, then the probability for each class is 0.1, therefore our loss is close to -log(0.1)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99YLWs6-xOEa","executionInfo":{"status":"ok","timestamp":1727663637409,"user_tz":360,"elapsed":9649,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"7fcd2498-de20-4037-c7a1-a83dd26b5f9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: 3.565006 analytic: 3.565006, relative error: 1.043934e-08\n","numerical: 1.134127 analytic: 1.134127, relative error: 5.403774e-08\n","numerical: 2.474679 analytic: 2.474679, relative error: 1.827674e-08\n","numerical: -4.153848 analytic: -4.153849, relative error: 1.199399e-08\n","numerical: 3.818440 analytic: 3.818440, relative error: 4.730559e-09\n","numerical: 1.621642 analytic: 1.621641, relative error: 2.622742e-08\n","numerical: 0.807105 analytic: 0.807105, relative error: 1.128870e-08\n","numerical: 0.165301 analytic: 0.165301, relative error: 3.688017e-07\n","numerical: -0.061166 analytic: -0.061166, relative error: 6.045153e-07\n","numerical: -0.256459 analytic: -0.256459, relative error: 3.185238e-09\n","numerical: -3.257964 analytic: -3.257964, relative error: 1.567313e-08\n","numerical: 0.965306 analytic: 0.965306, relative error: 1.225586e-08\n","numerical: -1.138860 analytic: -1.138860, relative error: 1.989816e-08\n","numerical: 0.386250 analytic: 0.386249, relative error: 1.267685e-07\n","numerical: -0.230471 analytic: -0.230471, relative error: 1.171739e-07\n","numerical: -0.801588 analytic: -0.801588, relative error: 9.261791e-09\n","numerical: -1.188344 analytic: -1.188344, relative error: 6.288479e-09\n","numerical: 1.081150 analytic: 1.081149, relative error: 7.296673e-08\n","numerical: -0.024536 analytic: -0.024536, relative error: 6.637974e-08\n","numerical: 0.211287 analytic: 0.211287, relative error: 3.969281e-07\n"]}],"source":["# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As we did for the SVM, use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","from cs6353.gradient_check import grad_check_sparse\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# similar to SVM case, do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhQWJnxsxOEa","executionInfo":{"status":"ok","timestamp":1727663637656,"user_tz":360,"elapsed":251,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"c9a04e6b-9f3e-47b2-c523-c79c6e1beb40"},"outputs":[{"output_type":"stream","name":"stdout","text":["naive loss: 2.342099e+00 computed in 0.320574s\n","vectorized loss: 2.342099e+00 computed in 0.017554s\n","Loss difference: 0.000000\n","Gradient difference: 0.000000\n"]}],"source":["# Now that we have a naive implementation of the softmax loss function and its gradient,\n","# implement a vectorized version in softmax_loss_vectorized.\n","# The two versions should compute the same results, but the vectorized version should be\n","# much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","\n","from cs6353.classifiers.softmax import softmax_loss_vectorized\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","\n","# As we did for the SVM, we use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXDOrO1axOEa","executionInfo":{"status":"ok","timestamp":1727673762351,"user_tz":360,"elapsed":523434,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"f3dd527a-661b-4ef0-dbd7-ad5d32f8e569"},"outputs":[{"output_type":"stream","name":"stdout","text":["lr 2.100000e-06 reg 4.500000e+04 train accuracy: 0.284694 val accuracy: 0.280000\n","lr 2.100000e-06 reg 4.577778e+04 train accuracy: 0.270490 val accuracy: 0.279000\n","lr 2.100000e-06 reg 4.655556e+04 train accuracy: 0.279612 val accuracy: 0.269000\n","lr 2.100000e-06 reg 4.733333e+04 train accuracy: 0.276571 val accuracy: 0.283000\n","lr 2.100000e-06 reg 4.811111e+04 train accuracy: 0.272653 val accuracy: 0.268000\n","lr 2.100000e-06 reg 4.888889e+04 train accuracy: 0.298837 val accuracy: 0.309000\n","lr 2.100000e-06 reg 4.966667e+04 train accuracy: 0.257184 val accuracy: 0.267000\n","lr 2.100000e-06 reg 5.044444e+04 train accuracy: 0.279143 val accuracy: 0.300000\n","lr 2.100000e-06 reg 5.122222e+04 train accuracy: 0.283061 val accuracy: 0.281000\n","lr 2.100000e-06 reg 5.200000e+04 train accuracy: 0.291347 val accuracy: 0.312000\n","lr 2.144444e-06 reg 4.500000e+04 train accuracy: 0.294388 val accuracy: 0.310000\n","lr 2.144444e-06 reg 4.577778e+04 train accuracy: 0.266490 val accuracy: 0.277000\n","lr 2.144444e-06 reg 4.655556e+04 train accuracy: 0.274408 val accuracy: 0.283000\n","lr 2.144444e-06 reg 4.733333e+04 train accuracy: 0.288469 val accuracy: 0.286000\n","lr 2.144444e-06 reg 4.811111e+04 train accuracy: 0.270735 val accuracy: 0.270000\n","lr 2.144444e-06 reg 4.888889e+04 train accuracy: 0.287082 val accuracy: 0.289000\n","lr 2.144444e-06 reg 4.966667e+04 train accuracy: 0.269184 val accuracy: 0.264000\n","lr 2.144444e-06 reg 5.044444e+04 train accuracy: 0.292143 val accuracy: 0.294000\n","lr 2.144444e-06 reg 5.122222e+04 train accuracy: 0.284449 val accuracy: 0.302000\n","lr 2.144444e-06 reg 5.200000e+04 train accuracy: 0.283592 val accuracy: 0.308000\n","lr 2.188889e-06 reg 4.500000e+04 train accuracy: 0.271020 val accuracy: 0.288000\n","lr 2.188889e-06 reg 4.577778e+04 train accuracy: 0.296020 val accuracy: 0.294000\n","lr 2.188889e-06 reg 4.655556e+04 train accuracy: 0.275918 val accuracy: 0.283000\n","lr 2.188889e-06 reg 4.733333e+04 train accuracy: 0.273347 val accuracy: 0.292000\n","lr 2.188889e-06 reg 4.811111e+04 train accuracy: 0.279633 val accuracy: 0.291000\n","lr 2.188889e-06 reg 4.888889e+04 train accuracy: 0.277837 val accuracy: 0.290000\n","lr 2.188889e-06 reg 4.966667e+04 train accuracy: 0.269857 val accuracy: 0.270000\n","lr 2.188889e-06 reg 5.044444e+04 train accuracy: 0.279286 val accuracy: 0.292000\n","lr 2.188889e-06 reg 5.122222e+04 train accuracy: 0.261429 val accuracy: 0.267000\n","lr 2.188889e-06 reg 5.200000e+04 train accuracy: 0.274980 val accuracy: 0.269000\n","lr 2.233333e-06 reg 4.500000e+04 train accuracy: 0.275571 val accuracy: 0.294000\n","lr 2.233333e-06 reg 4.577778e+04 train accuracy: 0.271510 val accuracy: 0.289000\n","lr 2.233333e-06 reg 4.655556e+04 train accuracy: 0.264959 val accuracy: 0.276000\n","lr 2.233333e-06 reg 4.733333e+04 train accuracy: 0.261245 val accuracy: 0.273000\n","lr 2.233333e-06 reg 4.811111e+04 train accuracy: 0.294612 val accuracy: 0.305000\n","lr 2.233333e-06 reg 4.888889e+04 train accuracy: 0.278306 val accuracy: 0.277000\n","lr 2.233333e-06 reg 4.966667e+04 train accuracy: 0.283245 val accuracy: 0.280000\n","lr 2.233333e-06 reg 5.044444e+04 train accuracy: 0.273510 val accuracy: 0.285000\n","lr 2.233333e-06 reg 5.122222e+04 train accuracy: 0.285490 val accuracy: 0.292000\n","lr 2.233333e-06 reg 5.200000e+04 train accuracy: 0.253082 val accuracy: 0.252000\n","lr 2.277778e-06 reg 4.500000e+04 train accuracy: 0.274714 val accuracy: 0.294000\n","lr 2.277778e-06 reg 4.577778e+04 train accuracy: 0.285633 val accuracy: 0.292000\n","lr 2.277778e-06 reg 4.655556e+04 train accuracy: 0.264490 val accuracy: 0.269000\n","lr 2.277778e-06 reg 4.733333e+04 train accuracy: 0.283673 val accuracy: 0.304000\n","lr 2.277778e-06 reg 4.811111e+04 train accuracy: 0.267980 val accuracy: 0.267000\n","lr 2.277778e-06 reg 4.888889e+04 train accuracy: 0.255327 val accuracy: 0.279000\n","lr 2.277778e-06 reg 4.966667e+04 train accuracy: 0.288306 val accuracy: 0.285000\n","lr 2.277778e-06 reg 5.044444e+04 train accuracy: 0.269224 val accuracy: 0.277000\n","lr 2.277778e-06 reg 5.122222e+04 train accuracy: 0.278898 val accuracy: 0.298000\n","lr 2.277778e-06 reg 5.200000e+04 train accuracy: 0.271000 val accuracy: 0.264000\n","lr 2.322222e-06 reg 4.500000e+04 train accuracy: 0.271551 val accuracy: 0.285000\n","lr 2.322222e-06 reg 4.577778e+04 train accuracy: 0.283959 val accuracy: 0.294000\n","lr 2.322222e-06 reg 4.655556e+04 train accuracy: 0.260449 val accuracy: 0.264000\n","lr 2.322222e-06 reg 4.733333e+04 train accuracy: 0.284714 val accuracy: 0.289000\n","lr 2.322222e-06 reg 4.811111e+04 train accuracy: 0.247980 val accuracy: 0.267000\n","lr 2.322222e-06 reg 4.888889e+04 train accuracy: 0.286531 val accuracy: 0.300000\n","lr 2.322222e-06 reg 4.966667e+04 train accuracy: 0.267367 val accuracy: 0.287000\n","lr 2.322222e-06 reg 5.044444e+04 train accuracy: 0.265816 val accuracy: 0.261000\n","lr 2.322222e-06 reg 5.122222e+04 train accuracy: 0.271265 val accuracy: 0.260000\n","lr 2.322222e-06 reg 5.200000e+04 train accuracy: 0.282367 val accuracy: 0.288000\n","lr 2.366667e-06 reg 4.500000e+04 train accuracy: 0.301020 val accuracy: 0.316000\n","lr 2.366667e-06 reg 4.577778e+04 train accuracy: 0.295510 val accuracy: 0.293000\n","lr 2.366667e-06 reg 4.655556e+04 train accuracy: 0.269306 val accuracy: 0.253000\n","lr 2.366667e-06 reg 4.733333e+04 train accuracy: 0.285959 val accuracy: 0.281000\n","lr 2.366667e-06 reg 4.811111e+04 train accuracy: 0.274347 val accuracy: 0.279000\n","lr 2.366667e-06 reg 4.888889e+04 train accuracy: 0.250041 val accuracy: 0.269000\n","lr 2.366667e-06 reg 4.966667e+04 train accuracy: 0.261122 val accuracy: 0.256000\n","lr 2.366667e-06 reg 5.044444e+04 train accuracy: 0.286571 val accuracy: 0.297000\n","lr 2.366667e-06 reg 5.122222e+04 train accuracy: 0.291490 val accuracy: 0.290000\n","lr 2.366667e-06 reg 5.200000e+04 train accuracy: 0.257224 val accuracy: 0.259000\n","lr 2.411111e-06 reg 4.500000e+04 train accuracy: 0.285469 val accuracy: 0.298000\n","lr 2.411111e-06 reg 4.577778e+04 train accuracy: 0.283939 val accuracy: 0.299000\n","lr 2.411111e-06 reg 4.655556e+04 train accuracy: 0.292510 val accuracy: 0.283000\n","lr 2.411111e-06 reg 4.733333e+04 train accuracy: 0.265939 val accuracy: 0.274000\n","lr 2.411111e-06 reg 4.811111e+04 train accuracy: 0.275224 val accuracy: 0.299000\n","lr 2.411111e-06 reg 4.888889e+04 train accuracy: 0.262612 val accuracy: 0.256000\n","lr 2.411111e-06 reg 4.966667e+04 train accuracy: 0.260939 val accuracy: 0.279000\n","lr 2.411111e-06 reg 5.044444e+04 train accuracy: 0.278469 val accuracy: 0.266000\n","lr 2.411111e-06 reg 5.122222e+04 train accuracy: 0.267388 val accuracy: 0.285000\n","lr 2.411111e-06 reg 5.200000e+04 train accuracy: 0.259816 val accuracy: 0.276000\n","lr 2.455556e-06 reg 4.500000e+04 train accuracy: 0.266041 val accuracy: 0.277000\n","lr 2.455556e-06 reg 4.577778e+04 train accuracy: 0.271980 val accuracy: 0.275000\n","lr 2.455556e-06 reg 4.655556e+04 train accuracy: 0.281245 val accuracy: 0.280000\n","lr 2.455556e-06 reg 4.733333e+04 train accuracy: 0.279755 val accuracy: 0.276000\n","lr 2.455556e-06 reg 4.811111e+04 train accuracy: 0.249224 val accuracy: 0.269000\n","lr 2.455556e-06 reg 4.888889e+04 train accuracy: 0.240837 val accuracy: 0.246000\n","lr 2.455556e-06 reg 4.966667e+04 train accuracy: 0.302388 val accuracy: 0.325000\n","lr 2.455556e-06 reg 5.044444e+04 train accuracy: 0.267510 val accuracy: 0.280000\n","lr 2.455556e-06 reg 5.122222e+04 train accuracy: 0.270714 val accuracy: 0.288000\n","lr 2.455556e-06 reg 5.200000e+04 train accuracy: 0.289000 val accuracy: 0.300000\n","lr 2.500000e-06 reg 4.500000e+04 train accuracy: 0.288878 val accuracy: 0.288000\n","lr 2.500000e-06 reg 4.577778e+04 train accuracy: 0.274837 val accuracy: 0.293000\n","lr 2.500000e-06 reg 4.655556e+04 train accuracy: 0.269898 val accuracy: 0.283000\n","lr 2.500000e-06 reg 4.733333e+04 train accuracy: 0.247653 val accuracy: 0.262000\n","lr 2.500000e-06 reg 4.811111e+04 train accuracy: 0.242041 val accuracy: 0.259000\n","lr 2.500000e-06 reg 4.888889e+04 train accuracy: 0.274286 val accuracy: 0.295000\n","lr 2.500000e-06 reg 4.966667e+04 train accuracy: 0.263449 val accuracy: 0.271000\n","lr 2.500000e-06 reg 5.044444e+04 train accuracy: 0.274286 val accuracy: 0.291000\n","lr 2.500000e-06 reg 5.122222e+04 train accuracy: 0.285531 val accuracy: 0.295000\n","lr 2.500000e-06 reg 5.200000e+04 train accuracy: 0.256184 val accuracy: 0.261000\n","best validation accuracy achieved during cross-validation: 0.325000\n","best learning rate and regularization strength (2.455555555555556e-06, 49666.666666666664)\n"]}],"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of over 0.35 on the validation set.\n","from cs6353.classifiers import Softmax\n","results = {}\n","best_val = -1\n","best_softmax = None\n","# learning_rates = [1e-7, 5e-7]\n","# # regularization_strengths = [2.5e4, 5e4]\n","# learning_rates = np.linspace(2.5e-6, 2.7e-6, num=10)\n","# # learning_rates = [2.666666666666667e-06]\n","# regularization_strengths = np.linspace(4.5e4, 5.5e4, num=10)\n","# # regularization_strengths = [52413.793103448275]\n","\n","learning_rates = np.linspace(2.1e-6, 2.5e-6, num=10)\n","regularization_strengths = np.linspace(4.5e4, 5.2e4, num=10)\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifier in best_softmax.                          #\n","################################################################################\n","lr_reg = []\n","for l_r in learning_rates:\n","    for reg in regularization_strengths:\n","        lr_reg.append((l_r, reg))\n","\n","for l_r in lr_reg:\n","    softmax = Softmax()\n","    softmax.train(X_train, y_train, l_r[0], l_r[1], num_iters=750, batch_size=200, verbose=False)\n","    y_train_pred = softmax.predict(X_train)\n","    y_val_pred = softmax.predict(X_val)\n","    train_accuracy = np.mean(y_train == y_train_pred)\n","    val_accuracy = np.mean(y_val == y_val_pred)\n","    if val_accuracy > best_val:\n","        best_val = val_accuracy\n","        best_softmax = softmax\n","        best_l_r = l_r\n","    results[l_r] = (train_accuracy, val_accuracy)\n","################################################################################\n","#                              END OF YOUR CODE                                #\n","################################################################################\n","\n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","\n","print('best validation accuracy achieved during cross-validation: %f' % best_val)\n","print('best learning rate and regularization strength', best_l_r)"]},{"cell_type":"markdown","metadata":{"id":"XX6_1L5WxOEa"},"source":["best validation accuracy achieved during cross-validation: 0.242000\n","best learning rate and regularization strength (2.8e-06, 100000.0)\n","\n","best validation accuracy achieved during cross-validation: 0.312000\n","best learning rate and regularization strength (2.689655172413793e-06, 44137.93103448275)\n","\n","best validation accuracy achieved during cross-validation: 0.302000\n","best learning rate and regularization strength (2.8e-06, 62758.620689655174)\n","\n","best validation accuracy achieved during cross-validation: 0.326000\n","best learning rate and regularization strength (2.666666666666667e-06, 52413.793103448275)\n","\n","best validation accuracy achieved during cross-validation: 0.296000\n","best learning rate and regularization strength (2.666666666666667e-06, 56206.89655172414)\n","\n","best validation accuracy achieved during cross-validation: 0.295000\n","best learning rate and regularization strength (2.666666666666667e-06, 50000.0)\n","\n","best validation accuracy achieved during cross-validation: 0.301000\n","best learning rate and regularization strength (2.666666666666667e-06, 53888.88888888889)\n","\n","best validation accuracy achieved during cross-validation: 0.304000\n","best learning rate and regularization strength (2.666666666666667e-06, 50555.555555555555)\n","\n","best validation accuracy achieved during cross-validation: 0.314000\n","best learning rate and regularization strength (2.6111111111111113e-06, 50555.555555555555)\n","\n","best validation accuracy achieved during cross-validation: 0.318000\n","best learning rate and regularization strength (2.6777777777777776e-06, 45000.0)\n","\n","best validation accuracy achieved during cross-validation: 0.331000\n","best learning rate and regularization strength (2.4444444444444447e-06, 48888.88888888889)\n","\n","best validation accuracy achieved during cross-validation: 0.314000\n","best learning rate and regularization strength (2.111111111111111e-06, 51111.11111111111)\n","\n","best validation accuracy achieved during cross-validation: 0.322000\n","best learning rate and regularization strength (2.144444444444444e-06, 46111.11111111111)\n","\n","best validation accuracy achieved during cross-validation: 0.331000\n","best learning rate and regularization strength (2.3666666666666667e-06, 52000.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IabSqWUrxOEa","outputId":"e73e82d7-2c48-44fd-cb43-a8d69c47a577"},"outputs":[{"name":"stdout","output_type":"stream","text":["softmax on raw pixels final test set accuracy: 0.307000\n"]}],"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"]},{"cell_type":"markdown","metadata":{"id":"k45TpXvNxOEa"},"source":["**Inline Question** - *True or False*\n","\n","It's possible to add a new data point to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n","\n","*Your answer*:\n","\n","**True**\n","\n","*Your explanation*:\n","\n","Because Softmax loss needs to sum all scores of all the classes, hence a new data points affects the value. Whereas SVM just takes the max between 0 and the difference of the scores which may have the chance that the new data point does not change the max value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZA7SIdLIxOEb"},"outputs":[],"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","\n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"]},{"cell_type":"code","source":["!jupyter nbconvert --to html /content/drive/MyDrive/MS/assignment2/softmax.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9IEKH1Dfvpm","executionInfo":{"status":"ok","timestamp":1727675744080,"user_tz":360,"elapsed":3537,"user":{"displayName":"Novella Alvina","userId":"07489434923825866656"}},"outputId":"f08f92cb-d90b-4b7a-f5fd-60562b639831"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/MS/assignment2/softmax.ipynb to html\n","[NbConvertApp] Writing 636293 bytes to /content/drive/MyDrive/MS/assignment2/softmax.html\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}